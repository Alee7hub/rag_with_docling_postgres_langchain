{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a1a1e0",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling.document_converter import DocumentConverter\n",
    "from langchain_core.documents import Document\n",
    "from docling.chunking import HybridChunker\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3f7d4",
   "metadata": {},
   "source": [
    "### Process all documents from a directory and save the resulted chunks to a list as langchain documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_documents_to_langchain(documents_dir: str, max_tokens: int = 512):\n",
    "    \"\"\"Process multiple documents and return a list of LangChain Document objects.\n",
    "    \n",
    "    Docling automatically handles all supported file formats (.pdf, .md, .docx, .html, .txt, etc.)\n",
    "    \n",
    "    Args:\n",
    "        documents_dir: Directory containing documents to process\n",
    "        max_tokens: Maximum tokens per chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of LangChain Document objects with page_content and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"BATCH HYBRID CHUNKING - TO LANGCHAIN DOCUMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get all files from directory (excluding directories)\n",
    "    documents_path = Path(documents_dir)\n",
    "    all_files = [f for f in documents_path.iterdir() if f.is_file()]\n",
    "    all_files = sorted(all_files)  # Sort for consistent ordering\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"\\nâœ— No files found in {documents_dir}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nFound {len(all_files)} documents to process\")\n",
    "    print(f\"Max tokens per chunk: {max_tokens}\\n\")\n",
    "    \n",
    "    # Initialize tokenizer once (reuse for all documents)\n",
    "    print(\"Initializing tokenizer...\")\n",
    "    model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Create chunker once (reuse for all documents)\n",
    "    chunker = HybridChunker(\n",
    "        tokenizer=tokenizer,\n",
    "        max_tokens=max_tokens,\n",
    "        merge_peers=True\n",
    "    )\n",
    "    \n",
    "    langchain_documents = []\n",
    "    total_chunks = 0\n",
    "    successful_docs = 0\n",
    "    failed_docs = []\n",
    "    \n",
    "    # Process each document\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            print(f\"\\nðŸ“„ Processing: {file_path.name}\")\n",
    "            \n",
    "            # Convert document\n",
    "            print(\"   Converting document...\")\n",
    "            converter = DocumentConverter()\n",
    "            result = converter.convert(str(file_path))\n",
    "            doc = result.document\n",
    "            \n",
    "            # Generate chunks\n",
    "            print(\"   Generating chunks...\")\n",
    "            chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "            chunks = list(chunk_iter)\n",
    "            \n",
    "            print(f\"   Creating {len(chunks)} LangChain Document objects...\")\n",
    "            \n",
    "            # Convert each chunk to LangChain Document\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Use contextualize to preserve headings and metadata\n",
    "                contextualized_text = chunker.contextualize(chunk=chunk)\n",
    "                \n",
    "                # Create LangChain Document with metadata\n",
    "                langchain_doc = Document(\n",
    "                    page_content=contextualized_text,\n",
    "                    metadata={\n",
    "                        \"source\": str(file_path),\n",
    "                        \"source_name\": file_path.name,\n",
    "                        \"chunk_index\": total_chunks + i,\n",
    "                        \"document_chunk_index\": i,\n",
    "                        \"total_chunks_in_document\": len(chunks)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                langchain_documents.append(langchain_doc)\n",
    "            \n",
    "            total_chunks += len(chunks)\n",
    "            successful_docs += 1\n",
    "            print(f\"   âœ“ Success! Total chunks so far: {total_chunks}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âœ— Error processing {file_path.name}: {e}\")\n",
    "            failed_docs.append(file_path.name)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ“ Successfully processed: {successful_docs}/{len(all_files)} documents\")\n",
    "    print(f\"âœ“ Total LangChain Documents created: {len(langchain_documents)}\")\n",
    "    \n",
    "    if failed_docs:\n",
    "        print(f\"\\nâœ— Failed documents ({len(failed_docs)}):\")\n",
    "        for doc in failed_docs:\n",
    "            print(f\"   - {doc}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LANGCHAIN DOCUMENTS READY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ“ Each chunk is a LangChain Document object\")\n",
    "    print(\"âœ“ page_content: Contextualized chunk text with headings\")\n",
    "    print(\"âœ“ metadata: source, source_name, chunk_index, etc.\")\n",
    "    print(\"âœ“ Ready for vector store ingestion (Chroma, FAISS, Pinecone, etc.)\")\n",
    "    \n",
    "    return langchain_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a68df9",
   "metadata": {},
   "source": [
    "### usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1097fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents_dir = \"../documents/raw\"\n",
    "all_chunks = process_documents_to_langchain(documents_dir=raw_documents_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd0e9b",
   "metadata": {},
   "source": [
    "### Vector storage -> Postgres/pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basics\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# PostgreSQL Configuration\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "POSTGRES_DB = os.getenv(\"POSTGRES_DB\")\n",
    "POSTGRES_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "\n",
    "# initiate embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Connection string\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{POSTGRES_USER}:{POSTGRES_PASSWORD}@localhost:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "# Initialize vector store\n",
    "vectorstore = PGVector(\n",
    "    connection=CONNECTION_STRING,\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"my_documents\",  # table name\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06936c86",
   "metadata": {},
   "source": [
    "### Injestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents\n",
    "vectorstore.add_documents(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c23b1",
   "metadata": {},
   "source": [
    "### Querying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c984579",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the Q1 2025 revenue target?\"\n",
    "\n",
    "# Query\n",
    "results = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "print(\"Retrieved Document:\")\n",
    "for doc in results:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=5)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the Q1 2025 revenue target?\"\n",
    "serialized, retrieved_docs = retrieve_context(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d20881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3035b5",
   "metadata": {},
   "source": [
    "### Vector store -> Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f50b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basics\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import langchain\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# import supabase\n",
    "from supabase.client import Client, create_client\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()  \n",
    "\n",
    "# initiate supabase db\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "# initiate embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# store chunks in vector store\n",
    "vector_store = SupabaseVectorStore.from_documents(\n",
    "    all_chunks,\n",
    "    embeddings,\n",
    "    client=supabase,\n",
    "    table_name=\"documents\",\n",
    "    query_name=\"match_documents\",\n",
    "    chunk_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b83b70",
   "metadata": {},
   "source": [
    "### Querying supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from supabase import Client, create_client\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# initiate embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# initiate supabase db\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "def query_vector_store(query: str, top_k: int = 5):\n",
    "    \"\"\"Query the Supabase vector store and return top_k similar documents.\n",
    "    \n",
    "    Args:\n",
    "        query: The input query string\n",
    "        top_k: Number of top similar documents to retrieve\n",
    "    \"\"\"\n",
    "    # 1. Embed the query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # 2. Query the Supabase vector store\n",
    "    resp = (\n",
    "        supabase.rpc(\n",
    "            \"match_documents\",\n",
    "            {\n",
    "                \"query_embedding\": query_embedding,\n",
    "                \"match_count\": top_k,\n",
    "                \"match_threshold\": 0.0,\n",
    "                \"filter\": {}  # optional jsonb filter\n",
    "            }\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    # if resp.raise_when_api_error():\n",
    "    #     raise Exception(resp.raise_when_api_error())\n",
    "\n",
    "    matches = resp.data  # list of rows returned by the function\n",
    "    for m in matches:\n",
    "        # similarity is included in the returned row (see function)\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Document ID: {m['id']}\\nSimilarity: {m['similarity']}\\nMetadata: {m['metadata']}\\nContent: {m['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the Q1 2025 revenue target?\"\n",
    "# query = \"When was NeuralFlow AI founded\"\n",
    "query = \"What ROI did GlobalFinance achieve?\"\n",
    "\n",
    "query_vector_store(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-docling-postgres",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
