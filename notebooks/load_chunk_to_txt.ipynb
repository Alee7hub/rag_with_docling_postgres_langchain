{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43d86f8",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b714d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b9eab",
   "metadata": {},
   "source": [
    "### Process all documents from a directory and save the resulted chunks to a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb2b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_chunks_to_file(chunks, chunker, output_path: str, document_name: str, chunk_offset: int = 0):\n",
    "    \"\"\"Append chunks from a document to an existing file.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunks to append\n",
    "        chunker: HybridChunker instance for contextualization\n",
    "        output_path: Path to output file\n",
    "        document_name: Name of the source document (for metadata)\n",
    "        chunk_offset: Starting chunk number (for continuous numbering across documents)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_path, 'a', encoding='utf-8') as f:\n",
    "        # Add document separator\n",
    "        f.write(f\"\\n{'#'*60}\\n\")\n",
    "        f.write(f\"# SOURCE DOCUMENT: {document_name}\\n\")\n",
    "        f.write(f\"{'#'*60}\\n\\n\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_number = chunk_offset + i\n",
    "            f.write(f\"{'='*60}\\n\")\n",
    "            f.write(f\"CHUNK {chunk_number}\\n\")\n",
    "            f.write(f\"Source: {document_name}\\n\")\n",
    "            f.write(f\"{'='*60}\\n\")\n",
    "\n",
    "            # Use contextualize to preserve headings and metadata\n",
    "            contextualized_text = chunker.contextualize(chunk=chunk)\n",
    "            f.write(contextualized_text)\n",
    "            f.write(\"\\n\\n\")\n",
    "    \n",
    "    return chunk_offset + len(chunks)\n",
    "\n",
    "def process_multiple_documents(documents_dir: str, output_file: str, max_tokens: int = 512):\n",
    "    \"\"\"Process multiple documents from a directory and save all chunks to a single file.\n",
    "    \n",
    "    Docling automatically handles all supported file formats (.pdf, .md, .docx, .html, .txt, etc.)\n",
    "    \n",
    "    Args:\n",
    "        documents_dir: Directory containing documents to process\n",
    "        output_file: Path to single output file for all chunks\n",
    "        max_tokens: Maximum tokens per chunk\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"BATCH HYBRID CHUNKING - MULTIPLE DOCUMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get all files from directory (excluding directories)\n",
    "    documents_path = Path(documents_dir)\n",
    "    all_files = [f for f in documents_path.iterdir() if f.is_file()]\n",
    "    all_files = sorted(all_files)  # Sort for consistent ordering\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"\\nâœ— No files found in {documents_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(all_files)} documents to process\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"Max tokens per chunk: {max_tokens}\\n\")\n",
    "    \n",
    "    # Initialize tokenizer once (reuse for all documents)\n",
    "    print(\"Initializing tokenizer...\")\n",
    "    model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Create chunker once (reuse for all documents)\n",
    "    chunker = HybridChunker(\n",
    "        tokenizer=tokenizer,\n",
    "        max_tokens=max_tokens,\n",
    "        merge_peers=True\n",
    "    )\n",
    "    \n",
    "    # Clear output file (start fresh)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{'#'*60}\\n\")\n",
    "        f.write(f\"# HYBRID CHUNKS - ALL DOCUMENTS\\n\")\n",
    "        f.write(f\"# Generated: {Path().absolute()}\\n\")\n",
    "        f.write(f\"# Total documents: {len(all_files)}\\n\")\n",
    "        f.write(f\"# Max tokens per chunk: {max_tokens}\\n\")\n",
    "        f.write(f\"{'#'*60}\\n\\n\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    successful_docs = 0\n",
    "    failed_docs = []\n",
    "    \n",
    "    # Process each document\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            print(f\"\\nðŸ“„ Processing: {file_path.name}\")\n",
    "            \n",
    "            # Convert document\n",
    "            print(\"   Converting document...\")\n",
    "            converter = DocumentConverter()\n",
    "            result = converter.convert(str(file_path))\n",
    "            doc = result.document\n",
    "            \n",
    "            # Generate chunks\n",
    "            print(\"   Generating chunks...\")\n",
    "            chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "            chunks = list(chunk_iter)\n",
    "            \n",
    "            # Append to output file\n",
    "            print(f\"   Appending {len(chunks)} chunks to output file...\")\n",
    "            total_chunks = append_chunks_to_file(\n",
    "                chunks=chunks,\n",
    "                chunker=chunker,\n",
    "                output_path=output_file,\n",
    "                document_name=file_path.name,\n",
    "                chunk_offset=total_chunks\n",
    "            )\n",
    "            \n",
    "            successful_docs += 1\n",
    "            print(f\"   âœ“ Success! Total chunks so far: {total_chunks}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âœ— Error processing {file_path.name}: {e}\")\n",
    "            failed_docs.append(file_path.name)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ“ Successfully processed: {successful_docs}/{len(all_files)} documents\")\n",
    "    print(f\"âœ“ Total chunks generated: {total_chunks}\")\n",
    "    print(f\"âœ“ Output file: {output_file}\")\n",
    "    \n",
    "    if failed_docs:\n",
    "        print(f\"\\nâœ— Failed documents ({len(failed_docs)}):\")\n",
    "        for doc in failed_docs:\n",
    "            print(f\"   - {doc}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"KEY BENEFITS OF HYBRID CHUNKING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ“ Respects document structure (sections, paragraphs)\")\n",
    "    print(\"âœ“ Token-aware (fits embedding model limits)\")\n",
    "    print(\"âœ“ Semantic coherence (doesn't split mid-sentence)\")\n",
    "    print(\"âœ“ Metadata preservation (headings, document context)\")\n",
    "    print(\"âœ“ Ready for RAG (optimized chunk sizes)\")\n",
    "    print(\"âœ“ All chunks in one file (easy to process for embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0ce24",
   "metadata": {},
   "source": [
    "### usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Process all documents from the raw directory\n",
    "raw_documents_dir = \"../documents/raw\"\n",
    "output_chunks_file = \"../documents/processed/all_chunks.txt\"\n",
    "\n",
    "# Process all documents (Docling handles all supported formats automatically)\n",
    "process_multiple_documents(\n",
    "    documents_dir=raw_documents_dir,\n",
    "    output_file=output_chunks_file,\n",
    "    max_tokens=512\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-docling-postgres",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
